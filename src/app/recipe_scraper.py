import json
import os
import requests
from bs4 import BeautifulSoup

from app.utils.image_utils import safe_filename
from app.utils.notion_utils import batch_import_to_notion
from config.settings import URLS, SCRAPED_PATH, RECIPES_DIR, IMAGES_DIR, CATEGORY_RULES


def fetch_recipe(url):
    headers = {
        "User-Agent": "Mozilla/5.0"
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    title_tag = soup.find('h1', class_='page-title')
    title = title_tag.get_text(strip=True) if title_tag else 'æœªçŸ¥èœå'

    intro_tag = soup.find('div', class_='desc mt10')
    intro = intro_tag.get_text(strip=True) if intro_tag else 'ï¼ˆæ— ç®€ä»‹ï¼‰'

    # ä¸»å›¾
    img = soup.select_one('div.cover.image.expandable img')
    image_url = img['src'] if img and 'src' in img.attrs else 'ï¼ˆæ— å›¾ç‰‡ï¼‰'

    ingredients = []
    ings_wrapper = soup.find('div', class_='ings')
    if ings_wrapper:
        for li in ings_wrapper.find_all('li'):
            name_tag = li.find('span', class_='name')
            amount_tag = li.find('span', class_='unit')
            name = name_tag.get_text(strip=True) if name_tag else ''
            amount = amount_tag.get_text(strip=True) if amount_tag else ''
            if name:
                ingredients.append(f"{name}ï¼š{amount}")

    steps = []
    step_tags = soup.select('div.steps > ol > li')
    for i, li in enumerate(step_tags, 1):
        step_text = li.get_text(strip=True)
        if step_text:
            steps.append(f"{i}. {step_text}")

    return {
        'èœå': title,
        'ç…§ç‰‡': image_url,
        'é£Ÿææ¸…å•': ingredients,
        'åšæ³•': steps,
        'é“¾æ¥': url,
        'ç±»å‹': classify_recipe(title),
    }


def load_scraped(path):
    if not os.path.exists(path):
        return set()
    with open(path, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f if line.strip())


def append_scraped(name, url, path):
    with open(path, "a", encoding="utf-8") as f:
        f.write(name + "\n")
        f.write(url + "\n")


def save_recipe_as_json(data, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    filename = safe_filename(data['èœå']) + ".json"
    filepath = os.path.join(output_dir, filename)
    try:
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ å·²ä¿å­˜ä¸º JSONï¼š{filepath}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ JSON å¤±è´¥ï¼š{e}")


def classify_recipe(name):
    """
    æ ¹æ®èœåè¿›è¡Œäº”å¤§ç±»åˆ†ç±»ï¼šè¤èœã€ç´ èœã€ç‚¹å¿ƒã€ä¸»é£Ÿã€æ±¤
    å¦‚æœéƒ½ä¸åŒ¹é…åˆ™è¿”å›â€œå…¶ä»–â€
    """
    categories = set()
    for category, keywords in CATEGORY_RULES.items():
        if any(kw in name for kw in keywords):
            categories.add(category)
            return list(categories)
    categories.add("å…¶ä»–")
    return list(categories)


if __name__ == '__main__':
    scraped_urls = load_scraped(SCRAPED_PATH)

    for url in URLS:
        if url in scraped_urls:
            print(f"âœ… å·²è·³è¿‡ï¼š{url}")
            continue

        print(f"\nğŸš€ æŠ“å–ä¸­ï¼š{url}")
        data = fetch_recipe(url)
        if not data:
            print(f"âŒ æŠ“å–å¤±è´¥ï¼š{url}")
            continue

        print(f"ğŸ½ï¸ èœåï¼š{data['èœå']}")
        print(f"ğŸ“· ç…§ç‰‡ï¼š{data['ç…§ç‰‡']}")
        print(f"ğŸ½ï¸ ç±»å‹ï¼š{data['ç±»å‹']}")
        print("ğŸ¥¬ é£Ÿææ¸…å•ï¼š")
        for ing in data['é£Ÿææ¸…å•']:
            print(f"  - {ing}")
        print("ğŸ‘£ åšæ³•ï¼š")
        for step in data['åšæ³•']:
            print(f"  {step}")
        # ä¸‹è½½ä¸»å›¾åˆ° images/
        # download_image(data['ç…§ç‰‡'], data['èœå'], IMAGES_DIR)
        save_recipe_as_json(data, RECIPES_DIR)

        append_scraped(data['èœå'], url, SCRAPED_PATH)

    # æ‰¹é‡å¯¼å…¥åˆ° Notion
    batch_import_to_notion(RECIPES_DIR)
